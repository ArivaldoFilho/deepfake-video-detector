{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a2733cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Importação das Bibliotecas\n",
    "##### os / time → manipulação de diretórios e utilidades gerais\n",
    "\n",
    "##### numpy → operações matemáticas e vetoriais\n",
    "\n",
    "##### cv2 (OpenCV) → leitura e processamento de imagens (Canny, FFT, HaarCascade etc.)\n",
    "\n",
    "##### joblib → salvar e carregar modelos (Logistic Regression, scaler, stacking)\n",
    "\n",
    "##### sklearn → modelos tradicionais e métricas\n",
    "\n",
    "##### tensorflow / keras → criação, treino e avaliação da CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0565f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22deaf16",
   "metadata": {},
   "source": [
    "### Configurações do Projeto\n",
    "\n",
    "##### Define caminhos para treino e validação.\n",
    "\n",
    "##### Ajusta parâmetros do modelo: tamanho da imagem, batch, número de épocas.\n",
    "\n",
    "##### Define extensões aceitas (incluindo WebP).\n",
    "\n",
    "##### Garante existência do diretório models/.\n",
    "\n",
    "##### Define os arquivos onde serão salvos os modelos treinados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1235fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"rvf10k/rvf10k\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "VALID_DIR = os.path.join(BASE_DIR, \"valid\")\n",
    "\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# formatos aceitos (inclui webp)\n",
    "IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".webp\")\n",
    "\n",
    "# diretórios de saída\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "CNN_MODEL_PATH = \"models/cnn_rvf10k_v2.keras\"\n",
    "DET_MODEL_PATH = \"models/det_logreg_v2.joblib\"\n",
    "STACK_MODEL_PATH = \"models/stacking_v2.joblib\"\n",
    "DET_SCALER_PATH = \"models/det_scaler_v2.joblib\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c6dd6e",
   "metadata": {},
   "source": [
    "##### Carrega automaticamente imagens organizadas em pastas por classe.\n",
    "\n",
    "##### Converte para batches já redimensionados em 128×128.\n",
    "\n",
    "##### Otimiza carregamento com cache() e prefetch()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_datasets():\n",
    "    train_ds = keras.utils.image_dataset_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"binary\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=IMG_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "    val_ds = keras.utils.image_dataset_from_directory(\n",
    "        VALID_DIR,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"binary\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=IMG_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_ds = train_ds.cache().shuffle(2000).prefetch(AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd62f76f",
   "metadata": {},
   "source": [
    "### Construção da CNN\n",
    "##### Normaliza a imagem para valores entre 0 e 1.\n",
    "\n",
    "### Bloco Convolucional\n",
    "##### Convolução (extração de padrões)\n",
    "\n",
    "##### BatchNorm (estabiliza treinamento)\n",
    "\n",
    "##### MaxPooling (reduz dimensionalidade)\n",
    "\n",
    "##### Os filtros aumentam a complexidade progressivamente.\n",
    "\n",
    "### Camadas finais da CNN\n",
    "##### GAP → reduz drasticamente parâmetros.\n",
    "\n",
    "##### Dropout → reduz overfitting.\n",
    "\n",
    "##### Dense(64) → representação aprendida.\n",
    "\n",
    "##### Sigmoid → saída entre 0 e 1 (probabilidade fake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c71b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape=(128, 128, 3)):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "\n",
    "    def block(x, filters):\n",
    "        x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D()(x)\n",
    "        return x\n",
    "\n",
    "    x = block(x, 32)\n",
    "    x = block(x, 64)\n",
    "    x = block(x, 128)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.6)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            keras.metrics.Precision(name=\"precision\"),\n",
    "            keras.metrics.Recall(name=\"recall\"),\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0fd424",
   "metadata": {},
   "source": [
    "### Extração de Features Determinísticas\n",
    "##### Usar o HaarCascade para detectar rosto antes da análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "def extract_face(gray):\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(30, 30))\n",
    "    if len(faces) == 0:\n",
    "        return cv2.resize(gray, IMG_SIZE)\n",
    "    x, y, w, h = max(faces, key=lambda f: f[2] * f[3])\n",
    "    return cv2.resize(gray[y : y + h, x : x + w], IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b26b8aa",
   "metadata": {},
   "source": [
    "### Local Binary Patterns\n",
    "##### Gera um histograma de textura com 256 valores, sendo muito útil para distinguir pele real de pele sintética."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca846f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbp_features(img):\n",
    "    img = img.astype(np.uint8)\n",
    "    h, w = img.shape\n",
    "    if h < 3 or w < 3:\n",
    "        img = cv2.resize(img, (max(3, w), max(3, h)))\n",
    "        h, w = img.shape\n",
    "\n",
    "    lbp = np.zeros((h - 2, w - 2), dtype=np.uint8)\n",
    "\n",
    "    for i in range(1, h - 1):\n",
    "        for j in range(1, w - 1):\n",
    "            center = img[i, j]\n",
    "            code = 0\n",
    "            code |= (img[i - 1, j - 1] > center) << 7\n",
    "            code |= (img[i - 1, j] > center) << 6\n",
    "            code |= (img[i - 1, j + 1] > center) << 5\n",
    "            code |= (img[i, j + 1] > center) << 4\n",
    "            code |= (img[i + 1, j + 1] > center) << 3\n",
    "            code |= (img[i + 1, j] > center) << 2\n",
    "            code |= (img[i + 1, j - 1] > center) << 1\n",
    "            code |= (img[i, j - 1] > center) << 0\n",
    "            lbp[i - 1, j - 1] = code\n",
    "\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=256, range=(0, 256), density=True)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab2a73",
   "metadata": {},
   "source": [
    "### Extração completa de features\n",
    "\n",
    "##### média / desvio padrão →\tintensidade geral da imagem\n",
    "##### Laplacian variance → mede nitidez / blur\n",
    "##### edge density →\tquantidade de bordas presentes\n",
    "##### FFT high frequency ratio → mede ruído e padrões artificiais\n",
    "##### LBP histogram →\ttextura local detalhada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80198ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_features_from_image(path):\n",
    "    img = cv2.imread(path)\n",
    "\n",
    "    if img is None:\n",
    "        raise ValueError(\n",
    "            f\"Falha ao carregar {path} — formato pode não ser suportado pelo OpenCV!\"\n",
    "        )\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face = extract_face(gray)\n",
    "\n",
    "    mean_intensity = np.mean(face)\n",
    "    std_intensity = np.std(face)\n",
    "    lap_var = cv2.Laplacian(face, cv2.CV_64F).var()\n",
    "    edges = cv2.Canny(face, 100, 200)\n",
    "    edge_density = np.mean(edges > 0)\n",
    "\n",
    "    f = np.fft.fft2(face)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    mag = np.abs(fshift)\n",
    "    h, w = mag.shape\n",
    "    cy, cx = h // 2, w // 2\n",
    "    low = mag[cy - h // 4 : cy + h // 4, cx - w // 4 : cx + w // 4].sum()\n",
    "    total = mag.sum() + 1e-8\n",
    "    hf_ratio = (total - low) / total\n",
    "\n",
    "    hist = lbp_features(face)\n",
    "\n",
    "    return np.concatenate(\n",
    "        [\n",
    "            np.array(\n",
    "                [mean_intensity, std_intensity, lap_var, edge_density, hf_ratio],\n",
    "                dtype=np.float32,\n",
    "            ),\n",
    "            hist.astype(np.float32),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfdc1d3",
   "metadata": {},
   "source": [
    "### Carregar dataset determinístico\n",
    "##### Percorre pastas /real e /fake, extrai features para cada imagem e retorna vetores X (features) e y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_det_dataset(base_dir):\n",
    "    X, y = [], []\n",
    "\n",
    "    for label, cls_name in [(0, \"real\"), (1, \"fake\")]:\n",
    "        cls_dir = os.path.join(base_dir, cls_name)\n",
    "\n",
    "        if not os.path.exists(cls_dir):\n",
    "            print(f\"[ERRO] Diretório não encontrado: {cls_dir}\")\n",
    "            continue\n",
    "\n",
    "        files = [f for f in os.listdir(cls_dir) if f.lower().endswith(IMG_EXTS)]\n",
    "\n",
    "        print(f\"[INFO] {cls_name}: {len(files)} imagens encontradas\")\n",
    "\n",
    "        for fname in files:\n",
    "            path = os.path.join(cls_dir, fname)\n",
    "            try:\n",
    "                features = deterministic_features_from_image(path)\n",
    "                X.append(features)\n",
    "                y.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Falhou: {path} → {e}\")\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f850c0",
   "metadata": {},
   "source": [
    "### Função de Avaliação\n",
    "##### Calcula Acurácia, Precisão, Recall, F1, Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209049d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(name, y_true, y_prob):\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "\n",
    "    print(f\"\\n==== {name} ====\")\n",
    "    print(f\"Acurácia : {acc:.4f}\")\n",
    "    print(f\"Precisão : {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1       : {f1:.4f}\")\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ec590",
   "metadata": {},
   "source": [
    "### Função Principal – main()\n",
    "##### Etapa 1 — Carregar datasets\n",
    "##### Etapa 2 — Inicializar CNN\n",
    "##### Etapa 3 — Treinar CNN\n",
    "##### Etapa 4 — Avaliar CNN\n",
    "##### Etapa 5 — Treinar modelo determinístico\n",
    "##### Etapa 6 — STACKING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd5983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    print(\"\\n[1] Carregando datasets CNN...\")\n",
    "    train_ds, val_ds = load_image_datasets()\n",
    "\n",
    "    print(\"\\n[2] Construindo CNN...\")\n",
    "    cnn = build_cnn_model()\n",
    "    cnn.summary()\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            patience=4, monitor=\"val_loss\", restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    print(\"\\n[3] Treinando CNN...\")\n",
    "    cnn.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)\n",
    "    cnn.save(CNN_MODEL_PATH)\n",
    "    print(f\"[OK] CNN salva em {CNN_MODEL_PATH}\")\n",
    "\n",
    "    print(\"\\n[4] Avaliando CNN...\")\n",
    "    y_true_cnn, y_prob_cnn = [], []\n",
    "    for Xb, yb in val_ds:\n",
    "        p = cnn.predict(Xb, verbose=0).flatten()\n",
    "        y_prob_cnn.extend(p)\n",
    "        y_true_cnn.extend(yb.numpy().flatten())\n",
    "\n",
    "    y_true_cnn = np.array(y_true_cnn).astype(int)\n",
    "    y_prob_cnn = np.array(y_prob_cnn)\n",
    "\n",
    "    evaluate(\"CNN v2\", y_true_cnn, y_prob_cnn)\n",
    "\n",
    "    print(\"\\n[5] Carregando dataset determinístico...\")\n",
    "    X_det_train, y_det_train = load_det_dataset(TRAIN_DIR)\n",
    "    X_det_val, y_det_val = load_det_dataset(VALID_DIR)\n",
    "\n",
    "    print(f\"[INFO] X_det_train: {X_det_train.shape}\")\n",
    "    print(f\"[INFO] X_det_val  : {X_det_val.shape}\")\n",
    "\n",
    "    if len(X_det_train) == 0:\n",
    "        raise RuntimeError(\n",
    "            \"❌ ERRO FATAL: Nenhuma imagem determinística carregada. Verifique extensões e leitura de arquivos.\"\n",
    "        )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_det_train_s = scaler.fit_transform(X_det_train)\n",
    "    X_det_val_s = scaler.transform(X_det_val)\n",
    "\n",
    "    det_model = LogisticRegression(max_iter=2000)\n",
    "    det_model.fit(X_det_train_s, y_det_train)\n",
    "\n",
    "    y_prob_det = det_model.predict_proba(X_det_val_s)[:, 1]\n",
    "    evaluate(\"Determinístico v2\", y_det_val, y_prob_det)\n",
    "\n",
    "    joblib.dump(det_model, DET_MODEL_PATH)\n",
    "    joblib.dump(scaler, DET_SCALER_PATH)\n",
    "\n",
    "    print(\"\\n[6] Preparando STACKING (CNN + Determinístico)...\")\n",
    "\n",
    "    def list_paths(base):\n",
    "        paths = []\n",
    "        labels = []\n",
    "        for label, cls in [(0, \"real\"), (1, \"fake\")]:\n",
    "            d = os.path.join(base, cls)\n",
    "            for f in sorted(os.listdir(d)):\n",
    "                if f.lower().endswith(IMG_EXTS):\n",
    "                    paths.append(os.path.join(d, f))\n",
    "                    labels.append(label)\n",
    "        return np.array(paths), np.array(labels)\n",
    "\n",
    "    val_paths, val_labels = list_paths(VALID_DIR)\n",
    "\n",
    "    # CNN alinhada a paths\n",
    "    y_prob_cnn_aligned = []\n",
    "    for p in val_paths:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            print(f\"[WARN] Falha ao reabrir {p} para CNN alinhada, pulando.\")\n",
    "            continue\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        rgb = cv2.resize(rgb, IMG_SIZE)\n",
    "        rgb = np.expand_dims(rgb, 0) / 255.0\n",
    "        y_prob_cnn_aligned.append(cnn.predict(rgb, verbose=0)[0][0])\n",
    "\n",
    "    # determinístico alinhado\n",
    "    X_det_val_aligned = []\n",
    "    for p in val_paths:\n",
    "        try:\n",
    "            feats = deterministic_features_from_image(p)\n",
    "            X_det_val_aligned.append(feats)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Falha ao extrair features alinhadas de {p}: {e}\")\n",
    "\n",
    "    X_det_val_aligned = np.array(X_det_val_aligned)\n",
    "    X_det_val_aligned_s = scaler.transform(X_det_val_aligned)\n",
    "    y_prob_det_aligned = det_model.predict_proba(X_det_val_aligned_s)[:, 1]\n",
    "\n",
    "    # Ajuste de tamanho caso alguma imagem tenha sido pulada\n",
    "    min_len = min(len(y_prob_cnn_aligned), len(y_prob_det_aligned), len(val_labels))\n",
    "    y_prob_cnn_aligned = np.array(y_prob_cnn_aligned[:min_len])\n",
    "    y_prob_det_aligned = np.array(y_prob_det_aligned[:min_len])\n",
    "    y_stack_labels = val_labels[:min_len]\n",
    "\n",
    "    # STACKING\n",
    "    X_stack = np.vstack([y_prob_cnn_aligned, y_prob_det_aligned]).T\n",
    "    stack = LogisticRegression()\n",
    "    stack.fit(X_stack, y_stack_labels)\n",
    "\n",
    "    y_prob_stack = stack.predict_proba(X_stack)[:, 1]\n",
    "    evaluate(\"Híbrido STACK v2\", y_stack_labels, y_prob_stack)\n",
    "\n",
    "    joblib.dump(stack, STACK_MODEL_PATH)\n",
    "    print(f\"[OK] STACK salvo em {STACK_MODEL_PATH}\")\n",
    "\n",
    "    print(\"\\nTreinamento concluído!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
